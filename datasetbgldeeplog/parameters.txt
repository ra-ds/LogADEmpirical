model_name: deeplog
dataset_name: None
device: cuda
data_dir: ./dataset/bgl
output_dir: ./datasetbgl
folder: bgl
log_file: ar01-fbx.bdx01_2024_07.log
sample_size: None
sample_log_file: None
parser_type: Drain
log_format: <<DateTime> <Router> <Daemon>(\[<Pid>\])?: <Content>>
regex: []
keep_para: False
st: 0.3
depth: 3
max_child: 100
tau: 0.5
is_process: False
is_instance: False
train_file: train_fixed100_instances.pkl
test_file: test_fixed100_instances.pkl
window_type: None
session_level: None
window_size: 5
step_size: 1
train_size: 0.4
train_ratio: 1
valid_ratio: 0.1
test_ratio: 1
max_epoch: 200
n_epochs_stop: 10
n_warm_up_epoch: 10
batch_size: 32
lr: 0.01
is_logkey: False
random_sample: False
is_time: False
min_freq: 1
seq_len: 10
min_len: 10
max_len: 512
mask_ratio: 0.5
adaptive_window: False
deepsvdd_loss: False
deepsvdd_loss_test: False
scale: None
hidden: 256
layers: 4
attn_heads: 4
num_workers: 5
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 0.0
sample: sliding_window
history_size: 10
embeddings: embeddings.json
sequentials: True
quantitatives: True
semantics: False
parameters: False
input_size: 1
hidden_size: 128
num_layers: 2
embedding_dim: 50
accumulation_step: 1
optimizer: adam
lr_decay_ratio: 0.1
num_candidates: 9
log_freq: 100
resume_path: False
num_encoder_layers: 1
num_decoder_layers: 1
dim_model: 300
num_heads: 8
dim_feedforward: 2048
transformers_dropout: 0.1
model_dir: ./datasetbgldeeplog/
train_vocab: ./datasetbgltrain.pkl
vocab_path: ./datasetbgldeeplog_vocab.pkl
model_path: ./datasetbgldeeplog/deeplog.pth
scale_path: ./datasetbgldeeplog/scale.pkl
